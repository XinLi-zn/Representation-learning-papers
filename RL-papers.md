

# Representation learning Papers

### Masked Image Modeling


+ **MAE** - Masked Autoencoders Are Scalable Vision Learners (CVPR 2022)  
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick   
[[paper](https://arxiv.org/abs/2111.06377)] [[code](https://github.com/facebookresearch/mae)]   


+ **SimMIM** - SimMIM: A Simple Framework for Masked Image Modeling (arXiv 2021-11)
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu  
[[paper](https://arxiv.org/abs/2111.09886)]  [[code](https://github.com/microsoft/SimMIM)]

+ **iBOT** - iBOT: Image BERT Pre-Training with Online Tokenizer  (ICLR 2022)   
Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, Tao Kong  
[[paper](https://arxiv.org/pdf/2111.07832.pdf)]  [[code](https://github.com/bytedance/ibot)]   

+ **DINO** - Emerging Properties in Self-Supervised Vision Transformers  (ICCV 2021)   
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin  
[[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf)]  [[code](https://github.com/facebookresearch/dino)]   

+ **EsViT** - Efficient Self-supervised Vision Transformers for Representation Learning  (arXiv 2021)   
Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, Han Hu  
[[paper](https://arxiv.org/abs/2106.09785)]    

+ **BEiT** - BEiT: BERT Pre-Training of Image Transformers  (arXiv 2021-6)   
Hangbo Bao, Li Dong, Furu Wei  
[[paper](https://arxiv.org/abs/2106.08254)]  [[code](https://github.com/microsoft/unilm/tree/master/beit)] 

+ **MST** - MST: Masked Self-Supervised Transformer for Visual Representation  (NeurIPS 2021)   
Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang  
[[paper](https://arxiv.org/abs/2106.05656)]
    <details> <summary>Notes</summary><img src="imgs/MST.png" width = "521" height = "262" alt="MST" align=center /> <div align=center>
    </div></details>
  

+ **VIT** - An image is worth 16x16 words: Transformers for image recognition at scale  (ICLR 2021)   
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby   
[[paper](https://arxiv.org/pdf/2010.11929.pdf)] [[code](https://github.com/google-research/vision_transformer)]
  <details>
    <summary>Notes</summary>
     <img src="imgs/sot/referformer.png" width = "552" height = "258" alt="referformer" align=center />  
    - Key idea:
         - 
     - Performance:
        - 
    </details>
### Contrastive Learning

+ **MoBY** - Self-Supervised Learning with Swin Transformers  (arXiv 2021)   
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, Jianfeng Gao  
[[paper](https://arxiv.org/abs/2106.09785)]  [[code](https://github.com/SwinTransformer/Transformer-SSL)] 

+ **MoCo_v3** - An Empirical Study of Training Self-Supervised Vision Transformers (ICCV 2021)  
Xinlei Chen, Saining Xie, Kaiming He   
[[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf)]  [[code](https://github.com/facebookresearch/moco-v3)]   


+ **SlowFast** - A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning (CVPR 2021)  
Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He   
[[paper](https://arxiv.org/abs/2104.14558)]  [[code](https://github.com/facebookresearch/SlowFast)]  

+ **SimSiam** - Exploring Simple Siamese Representation Learning (CVPR 2021)  
Xinlei Chen and Kaiming He   
[[paper](https://arxiv.org/pdf/2011.10566.pdf)]  [[code](https://github.com/facebookresearch/simsiam)]  

+ **MoCo_v2** - Improved Baselines with Momentum Contrastive Learning (arXiv, 2020-3)  
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He   
[[paper](https://arxiv.org/pdf/2003.04297.pdf)]  [[code](https://github.com/facebookresearch/moco)]   

+ **BYOL** - Bootstrap your own latent: A new approach to self-supervised learning  (NeurIPS 2020) 
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, Michal Valko
[[paper](https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf)]  [[code](https://github.com/deepmind/deepmind-research/tree/master/byol)]  

+ **SwAV** - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (NeurIPS 2020)  
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin  
[[paper](https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf)]  [[code]( https://github.com/facebookresearch/swav)]   

+ **MoCo_v1** - Momentum Contrast for Unsupervised Visual Representation Learning (CVPR 2020)  
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick   
[[paper](https://arxiv.org/abs/1911.05722)]  [[code](https://github.com/facebookresearch/moco)]   

+ **SimCLR** - A simple framework for contrastive learning of visual representations (PMLR 2020) 
Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton  
[[paper](https://arxiv.org/abs/2002.05709)]  [[code](https://github.com/google-research/simclr)]  




